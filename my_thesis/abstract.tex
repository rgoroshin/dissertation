Much of computer vision has been devoted to the question of representation
through feature extraction.  Useful features transform the raw pixel values to
a representation in which common problems such as identification, tracking, and
segmentation of objects are easier to solve. Recently, deep feature hierarchies
have proven to be immensely successful at solving many problems in computer
vision. For specific problems, these hierarchies are learned from the input
data itself along with problem-specific label information by minimizing an
objective function.  Recent findings suggest that the learned features are not
problem specific and can be transfered across multiple visual tasks, suggesting
that there exists a generically useful feature representation for natural
visual data.    

This work aims to uncover the principles that lead to these generic feature
representations without resorting to problem specific label information. 
