\begin{table} 
\begin{center} 
\begin{tabular}{|c||c|c|c|c|}
\hline
\emph{Algorithm} & \emph{Manifold Model} & \emph{Encode} & \emph{Decode} & \emph{Relate Enc. \& Dec.}\\
%\thickhline
\hline \hline 
\cellcolor{red}PCA & Linear &\checkmark & \checkmark & $W_D = W_E ^T$\\
\hline
\cellcolor{red}ICA & Linear &\checkmark & \checkmark & $W_D = W_E ^T$\\
\hline
\cellcolor{yellow}Sparse Coding & Local Linear &\checkmark & \checkmark & Separate \\
\hline 
\cellcolor{yellow}PSD \& LISTA & Local Linear &\checkmark & \checkmark & Separate\\
\hline
\cellcolor{lightblue}DrLIM & Nonlinear & \checkmark & X & Enc. Only\\
\hline
\cellcolor{lightblue}Chapter \ref{chapter:slow} & Nonlinear & \checkmark & X & Enc. Only\\
\hline
\cellcolor{lightblue}Chapter \ref{chapter:linear} & Nonlinear & \checkmark & \checkmark & Separate\\
\hline
Adversarial Networks & Nonlinear & X & \checkmark & Dec. Only \\
\hline 
\cellcolor{green}Auto-Encoders & Nonlinear & \checkmark & \checkmark & Separate \\
\hline 
\end{tabular} \\
\vspace{0.25cm} \hspace{0.25cm}  
\begin{tabular}{|c|}
\hline 
\emph{Model Objective/Prior}\\  
\hline \hline
\cellcolor{red} De-correlation/Independence  \\
\hline
\cellcolor{yellow} Sparsity \\
\hline
\cellcolor{lightblue} Metric Learning/Geometric Prior  \\
\hline
\cellcolor{green} All of the Above \\
\hline
None \\
\hline
\end{tabular}
\end{center}
\caption{Summary of unsupervised feature learning algorithms and their properties} 
\label{tbl:models} 
\end{table} 

Unsupervised feature learning arguably dates back to the invention of Principal
Component Analysis (PCA) in 1901 by Karl Pearson \cite{PCA}. As mentioned in
Chapter \ref{chapter:introduction}, feature learning algorithms model
intrinsically low dimensional data embedded in a high dimensional ambient
space. These models can usually be decomposed into two parts: a mapping from
the input space to the feature space, called encoding, and mapping the feature
space back to the input space, called decoding. If the encoding or decoding
processes have corresponding functional forms, they are referred to as the
``encoder'' and ``decoder'', respectively.  From the natural image manifold
perspective, ideal encodings map the extrinsic coordinates (i.e. pixel values)
to intrinsic manifold coordinates. It is hoped that these intrinsic coordinates
correspond to physical attributes of the natural world, such as the presence of
certain objects in the scene, their properties, etc \cite{nair2008,capsules}.
Unsupervised feature learning models are mainly distinguished by: i-the
geometrical prior they assume about the data manifold, ii-whether they learn an
encoder, decoder, or both. For example, Principal Component Analysis assumes
that the data is concentrated around a hyper-plane, i.e. it assumes a globally
linear data manifold model. The PCA encoder is a matrix operator, $W_e$, as is
the decoder $W_d=W_e^T$.  Obtaining ``invariant'' representations has been a
major driving force in feature learning, driven mainly by recognition and
classification problems.  Invariant features imply that the encoding process is
necessarily a many-to-one mapping. This means that the decoding process must
involve some random selection among the possible inputs that produced the code,
usually  by sampling from a distribution. Models that include a decoder are
called ``generative models'' in the literature. 

Table \ref{tbl:models} summarizes key aspects of several well-known feature
learning models, as well as the new models which will be introduced in Chapters
\ref{chapter:slow} and \ref{chapter:linear}. The first column lists the name of
the model, the color indicates the type of objective each model tries to
optimize.  These are summarized in the table below. For example, one version of
PCA finds maximally decorrelated linear components. The ``manifold model''
column indicates the geometric prior each model assumes about the data
manifold. The next two columns indicate whether each model learns an encoder
and/or decoder.  Finally, the last column summarizes the relationship enforced
between the encoder and decoder. For example in PCA the encoder and decoder are
related by the transpose operator. This chapter will review the algorithms in
Table \ref{tbl:models}, placing emphasis on the precursors of the algorithms
presented in Chapters \ref{chapter:slow} and \ref{chapter:linear}. The
algorithms will be presented as various models of the data manifold.

\section{Principal and Independent Component Analysis} 

\begin{figure} 
\centering
\includegraphics[scale=0.4]{./figures/related_work/ICA.png} 
\includegraphics[scale=0.4]{./figures/related_work/L1.png} 
\caption{Left: visualization of the optimization problem used to solve for independent representations. Right: visualization of the optimization problem used to solve for sparse representations.} 
\label{fig:ICA_lasso} 
\end{figure}  

Principal Component Analysis (PCA) and Independent Component Analysis (ICA) are
the most well know linear manifold models \cite{ICA}. Both assume that the
observed high dimensional data $x$ are generated from some low dimensional
latent variables $z$ via a linear operator $A$, that is $x=Az$. Assuming that
the data is zero mean, PCA implicitly makes the assumption that the latent
variables correspond to directions with the largest variance \cite{PCA}.  These
components can be obtained from the Eigen decomposition of the covariance
matrix.  ICA however searches for linearly independent components, formulating
the definition of independence using the central limit theorem. Although the
detailed derivation of ICA will not be presented here, we will mention that it
leads to a fourth order moment (kurtosis) maximization problem subject to a
second order moment (variance) constraint. This optimization problem is
visualized as the left plot of Figure \ref{fig:ICA_lasso}. The blue curves
represent the level sets of the kurtosis objective, and the red curve
represents the unit variance constraint. Figure \ref{fig:ICA_features}  shows
the features learned using an ICA algorithm on natural image patches.  Note the
strong resemblance to the bases learned with sparse feature learning algorithms
such as Sparse Coding \cite{SC}. The next section will establish a remarkable
connection between ICA and sparse inference.  

\begin{figure} 
\centering
\includegraphics[scale=0.4]{./figures/related_work/imageManifold.png} 
\caption{Sparse coding learns a local linear model of the data manifold (image from \cite{SC2}).}
\label{fig:ICA_features} 
\end{figure} 

\begin{figure} 
\centering
\includegraphics[scale=0.4]{./figures/related_work/ICABasis.png} 
\caption{Independent features learned on natural image patches}
\label{fig:ICA_features} 
\end{figure} 

\section{Sparse Representations} 
Sparse inference refers to the problem of finding the coefficients $z$ which
reconstruct the input $x$ as a sparse linear combination of some basis elements
contained an over-complete dictionary $W_d$. Sparsity is measured by the number
of non-zero coefficients; the higher the sparsity the fewer non-zero
coefficients in $z$ used to represent $x$.  
The sparse inference problem can be stated formally as: 

\begin{equation} 
min \|z\|_0 \mbox{ subject to } x = W_dz 
\end{equation} 

In general, this is an intractable combinatorial problem. A now famous
breakthrough showed that exact sparse solutions can be obtained, under certain
conditions, by replacing the $L_0$ norm with an $L_1$ norm \cite{candes2006}.
This optimization problem is depicted on the right side of Figure
\ref{fig:ICA_lasso}.  The level sets of the $L_1$ norm are shown in blue and
the linear constraint is shown in red. The minimum occur on the $x$-axis, with
$y=0$ implying that the solution is indeed sparse. Remarkably, the solution to
the ICA problem (left side of Figure \ref{fig:ICA_lasso}) is also sparse though the
objective was explicitly derived with sparsity in mind. Given this similarity 
between ICA and sparse feature learning, it is not surprising that the features 
learned with ICA on natural images strongly resemble those learned with sparse coding. 

Relaxing the constraint, this problem can be written as an unconstrained loss
functional called the lasso (also basis pursuit de-noising) \cite{BP}:  

\begin{equation} 
L_{lasso} = \frac{1}{2}\|x-W_dz\|^2_2 + \alpha |z|_1
\label{eqn:lasso} 
\end{equation} 

Minimizing the above loss with respect to $z$ and a fixed $W_d$ corresponds to
the sparse inference problem, and corresponds to the ``encoding''. Decoding
sparse representations is trivial, it simply corresponds to computing $W_dz$.
The loss described by the above equation is non-convex with respect to $z$,
however each of the two terms are individually convex which prompted
alternating minimization approaches such as the various Iterative Shrinkage and
Thresholding algorithms \cite{FISTA}. It is possible to find ``feed forward'',
i.e. non-iterative approximations (i.e. encoders) to iterative sparse inference
algorithms for data $x$ drawn from a fixed distribution. Thus the codes are
given by a functional mapping of the input, i.e. $z=F_w(x)$. 

\section{Metric Learning}

\section{Auto-Encoders and Energy Based Learning} 
 
