\relax 
\bibstyle{biblatex}
\bibdata{main-blx,references}
\citation{biblatex-control} 
\@writefile{toc}{\boolfalse{citerequest}\boolfalse{citetracker}\boolfalse{pagetracker}}
\@writefile{lof}{\boolfalse{citerequest}\boolfalse{citetracker}\boolfalse{pagetracker}}
\@writefile{lot}{\boolfalse{citerequest}\boolfalse{citetracker}\boolfalse{pagetracker}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {section}{Dedication}{ii}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {section}{Acknowledgements}{iii}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {section}{Abstract}{iv}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {section}{List of Figures}{vii}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {section}{List of Tables}{xi}}
\citation{simoncelli2001}
\citation{bengio2013}
\citation{tenenbaum2000}
\citation{roweis2000}
\citation{goroshin2015}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter{refsection}{0}}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:introduction}{{1}{1}}
\citation{SIFT}
\citation{HoG}
\citation{gist}
\citation{fukushima1980}
\citation{LeCun1998}
\citation{ImageNet}
\citation{LeCun1998}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Top left: random image, Top right: natural image, Bottom left: visualization of unstructured data, Bottom right: visualization of data with intrinsic structure\relax }}{2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:structure}{{1.1}{2}}
\citation{hubel1968}
\citation{felleman1991}
\citation{yamins2014}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces The LeNet5 network was trained to recognize hand written characters in order to automatically process bank checks\relax }}{3}}
\newlabel{fig:LeNet5}{{1.2}{3}}
\citation{foldiak1991}
\citation{sinha2013}
\citation{sinha2013}
\citation{sharma2000}
\citation{yosinski2014}
\citation{SAE}
\citation{CAE}
\citation{LISTA}
\citation{SFA}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Thesis Outline and Summary of Contributions}{4}}
\citation{PCA}
\citation{nair2008}
\citation{capsules}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Related Work}{6}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter{refsection}{0}}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:related_work}{{2}{6}}
\@writefile{lot}{\defcounter{refsection}{0}}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces Summary of unsupervised feature learning algorithms and their properties\relax }}{7}}
\newlabel{tbl:models}{{2.1}{7}}
\citation{ICA}
\citation{PCA}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Left: visualization of the optimization problem used to solve for independent representations. Right: visualization of the optimization problem used to solve for sparse representations.\relax }}{8}}
\newlabel{fig:ICA_lasso}{{2.1}{8}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Principal and Independent Component Analysis}{8}}
\citation{candes2006}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Sparse Representations}{9}}
\citation{BP}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Metric Learning}{10}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Auto-Encoders and Energy Based Learning}{10}}
\citation{DHS}
\citation{SC}
\citation{LISTA}
\citation{SAE1}
\citation{SAE2}
\citation{CAE}
\citation{DAE}
\citation{DHS}
\citation{CAE}
\citation{SAE1}
\citation{SAE2}
\citation{bengio_new}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Saturating Auto-Encoders}{11}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter{refsection}{0}}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:SATAE}{{3}{11}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Introduction}{11}}
\citation{SAE1}
\citation{SAE2}
\citation{CAE}
\citation{CAE}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Latent State Regularization}{12}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Saturating Auto-Encoder through Complementary Nonlinearities}{13}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Three nonlinearities (top) with their associated complementary regularization functions(bottom).\relax }}{15}}
\newlabel{fig:nonlin}{{3.1}{15}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Effect of the Saturation Regularizer}{16}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Visualizing the Energy Landscape}{16}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Energy surfaces for unregularized (left), and regularized (right) solutions obtained on SATAE-shrink and 10 basis vectors. Black corresponds to low reconstruction energy. Training points lie on a one-dimensional manifold shown in yellow.\relax }}{17}}
\newlabel{fig:toyshrink}{{3.2}{17}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}SATAE-shrink}{17}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces SATAE-SL toy example with two basis elements. Top Row: three randomly initialized solutions obtained with no regularization. Bottom Row: three randomly initialized solutions obtained with regularization.\relax }}{18}}
\newlabel{fig:toysatlinear}{{3.3}{18}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Geometric visualization of non-linearities\relax }}{18}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Evolution of two filters with increasing saturation regularization for a SATAE-SL trained on CIFAR-10. Filters corresponding to larger values of $\alpha $ were initialized using the filter corresponding to the previous $\alpha $. The regularization parameter was varied from 0.1 to 0.5 (left to right) in the top five images and 0.5 to 1 in the bottom five \relax }}{19}}
\newlabel{fig:horse}{{3.5}{19}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 5\p@ plus2.5\p@ minus\p@ \topsep 10\p@ plus4\p@ minus6\p@ \itemsep 5\p@ plus2.5\p@ minus\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Basis elements learned by the SATAE using different nonlinearities on: 28x28 binary MNIST digits, 12x12 gray scale natural image patches, and CIFAR-10. (a) SATAE-shrink trained on MNIST, (b) SATAE-saturated-linear trained on MNIST, (c) SATAE-shrink trained on natural image patches, (d) SATAE-saturated-linear trained on natural image patches, (e)-(f) SATAE-shrink trained on CIFAR-10 with $\alpha =0.1$ and $\alpha =0.5$, respectively, (g)-(h) SATAE-SL trained on CIFAR-10 with $\alpha =0.1$ and $\alpha =0.6$, respectively. \relax }}{20}}
\newlabel{fig:results}{{3.6}{20}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}SATAE-saturated-linear}{21}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.4}Experiments on CIFAR-10}{21}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Experimental Details}{22}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Discussion}{23}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.1}Extension to Differentiable Functions}{24}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces Illustration of the complimentary function ($f_c$) as defined by Equation 3 for a non-monotonic activation function ($f$). The absolute derivative of $f$ is shown for comparison.\relax }}{25}}
\newlabel{fig:diff_cc}{{3.7}{25}}
\citation{CAE}
\citation{LISTA}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.2}Relationship with the Contractive Auto-Encoder}{26}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.3}Relationship with the Sparse Auto-Encoder}{26}}
\citation{SC}
\citation{SAE}
\citation{SAE2}
\citation{ConvSC}
\citation{FISTA}
\citation{LISTA}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Convolutional Sparse Inference}{28}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter{refsection}{0}}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:LISTA}{{4}{28}}
\citation{ConvSC}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces LISTA network architecture\relax }}{29}}
\newlabel{fig:LISTA}{{4.1}{29}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Convolutional-LISTA}{29}}
\newlabel{eqn:lasso}{{4.1}{29}}
\citation{groupSparsity}
\citation{LISTA}
\citation{LISTA}
\citation{groupSparsity}
\citation{ConvSC}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Learning to Perform Sparse Inference}{30}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Pre-trained decoder used for fixed-decoder experiments\relax }}{31}}
\newlabel{fig:FISTA_decoder}{{4.2}{31}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Whitened CIFAR-10 samples (top) and their corresponding reconstructions.\relax }}{31}}
\newlabel{fig:CIFAR_rec}{{4.3}{31}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Sparse codes obtained with FISTA inference\relax }}{32}}
\newlabel{fig:activations}{{4.4}{32}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Sparse Inference in a Fixed Dictionary}{33}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces Sparse inference learning curves\relax }}{34}}
\newlabel{fig:learning_curves}{{4.5}{34}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces Lasso Loss with Fixed Dictionary\relax }}{35}}
\newlabel{fig:lasso_loss_fixed}{{4.6}{35}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces Reconstruction and Sparsity with Fixed Dictionary\relax }}{35}}
\newlabel{fig:scatter_fixed}{{4.7}{35}}
\citation{SAE}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces Lasso Loss with Learned Dictionary\relax }}{36}}
\newlabel{fig:lasso_loss_learned}{{4.8}{36}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Sparse Convolutional Auto-encoders}{36}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.9}{\ignorespaces Reconstruction and Sparsity with Learned Dictionary\relax }}{37}}
\newlabel{fig:scatter_learned}{{4.9}{37}}
\citation{SFA}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Learning Spatiotemporally Coherent Metrics}{38}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter{refsection}{0}}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:slow}{{5}{38}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Introduction}{38}}
\citation{DrLIM}
\citation{DrLIMVideo}
\citation{SFA}
\citation{Bengio2012}
\citation{CAE}
\citation{DAE}
\citation{SATAE}
\citation{LeCun1998}
\citation{JoanScat}
\citation{SFA}
\citation{SSA}
\citation{hyvarinen2003bubbles}
\citation{SFA}
\citation{complexCells}
\citation{complexCells}
\citation{DrLIM}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Contributions and Prior Work}{40}}
\citation{groupSparsity}
\citation{complexCells}
\citation{Cadieu}
\citation{zou2012deep}
\citation{Huyvarinen}
\citation{SFA}
\citation{SSA}
\citation{complexCells}
\citation{DrLIM}
\newlabel{fig:toyplane}{{5.1a}{42}}
\newlabel{sub@fig:toyplane}{{a}{42}}
\newlabel{fig:drlim}{{5.1b}{42}}
\newlabel{sub@fig:drlim}{{b}{42}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces (a) Three samples from our rotating plane toy dataset. (b) Scatter plot of the dataset plotted in the output space of $G_W$ at the start (top) and end (bottom) of training. The left side of the figure is colored by the yaw angle, and the right side by roll, $0^{\circ }$ blue, $90^{\circ }$ in pink.\relax }}{42}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Slowness as Metric Learning }{42}}
\newlabel{sec:slowmetric}{{5.3}{42}}
\citation{siamese}
\newlabel{eqn:drlimcrit}{{5.1}{43}}
\citation{JoanScat}
\citation{LeCun1998}
\citation{ImageNet}
\citation{JoanPooling}
\citation{groupSparsity}
\citation{LISTA}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Slow Feature Pooling Auto-Encoders}{44}}
\newlabel{sfautoencs}{{5.4}{44}}
\newlabel{eqn:loss}{{5.2}{45}}
\citation{LeCun1998}
\citation{MaxOut}
\newlabel{fig:pooldec}{{5.2a}{46}}
\newlabel{sub@fig:pooldec}{{a}{46}}
\newlabel{fig:pooll1dec}{{5.2b}{46}}
\newlabel{sub@fig:pooll1dec}{{b}{46}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Pooled decoder dictionaries learned without (a) and with (b) the $L_1$ penalty using (6.1\hbox {}).\relax }}{46}}
\newlabel{fig:sfpool}{{5.2}{46}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.1}Fully-Connected Architecture}{46}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.2}Convolutional Architecture}{47}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Block diagram of the Siamese convolutional model trained on pairs of frames. \relax }}{48}}
\newlabel{fig:diagram}{{5.3}{48}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Experimental Results}{48}}
\citation{alexthesis}
\citation{Bengio2012}
\citation{groupSparsity}
\newlabel{eqn:groupL1}{{5.3}{50}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces Six scenes from our YouTube dataset \relax }}{51}}
\newlabel{fig:youtube}{{5.4}{51}}
\newlabel{fig:drlimfilters}{{5.5a}{51}}
\newlabel{sub@fig:drlimfilters}{{a}{51}}
\newlabel{fig:L1filters}{{5.5b}{51}}
\newlabel{sub@fig:L1filters}{{b}{51}}
\newlabel{fig:groupL1filters}{{5.5c}{51}}
\newlabel{sub@fig:groupL1filters}{{c}{51}}
\newlabel{fig:slowfilters}{{5.5d}{51}}
\newlabel{sub@fig:slowfilters}{{d}{51}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces Pooled convolutional dictionaries (decoders) learned with: (a) DrLIM and (b) sparsity only, (c) group sparsity, and (d) sparsity and slowness. Groups of four features that were pooled together are depicted as horizontally adjacent filters.\relax }}{51}}
\newlabel{fig:filters}{{5.5}{51}}
\newlabel{fig:videoquery}{{5.6a}{52}}
\newlabel{sub@fig:videoquery}{{a}{52}}
\newlabel{fig:cifarquery}{{5.6b}{52}}
\newlabel{sub@fig:cifarquery}{{b}{52}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces Query results in the (a) video and (b) CIFAR-10 datasets. Each row corresponds to a different feature space in which the queries were performed; numbers (1 or 2) denote the number of convolution-pooling layers. \relax }}{52}}
\newlabel{fig:query}{{5.6}{52}}
\newlabel{fig:query}{{5.6}{52}}
\newlabel{fig:ROCtime}{{5.7a}{52}}
\newlabel{sub@fig:ROCtime}{{a}{52}}
\newlabel{fig:ROCCIFAR}{{5.7b}{52}}
\newlabel{sub@fig:ROCCIFAR}{{b}{52}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces Precision-Recall curves corresponding to the YouTube (a) and CIFAR-10 (b) dataset.\relax }}{52}}
\newlabel{fig:ROC}{{5.7}{52}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {section}{\numberline {5.6}Conclusion}{54}}
\citation{imageNetTransfer}
\citation{supFromTracker}
\citation{FBvideo}
\citation{predAlexNet}
\citation{Bengio2012}
\citation{sparseCoding}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Learning to Linearize under Uncertainty}{56}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter{refsection}{0}}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:linear}{{6}{56}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Introduction}{56}}
\citation{FBvideo}
\citation{supFromTracker}
\citation{predAlexNet}
\citation{capsules}
\citation{taco}
\citation{FBvideo}
\citation{supFromTracker}
\citation{predAlexNet}
\citation{slowAE}
\citation{predAlexNet}
\citation{ImageNet}
\citation{supFromTracker}
\citation{slowAE}
\citation{SSA}
\citation{Cadieu}
\citation{SFA}
\citation{DrLIMVideo}
\citation{FBvideo}
\citation{ranzato2007}
\citation{zeiler2010}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Prior Work}{58}}
\newlabel{sec:prior work}{{6.2}{58}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Learning Linearized Representations}{59}}
\newlabel{sec:main}{{6.3}{59}}
\newlabel{fig:3pixels}{{6.1a}{60}}
\newlabel{sub@fig:3pixels}{{a}{60}}
\newlabel{fig:manifold}{{6.1b}{60}}
\newlabel{sub@fig:manifold}{{b}{60}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces (a) A video generated by translating a Gaussian intensity bump over a three pixel array ($x$,$y$,$z$), (b) the corresponding manifold parametrized by time in three dimensional space\relax }}{60}}
\newlabel{fig:threepixel}{{6.1}{60}}
\newlabel{eqn:loss}{{6.1}{60}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.1}Phase Pooling}{60}}
\newlabel{subsec:phase pooling}{{6.3.1}{60}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces The basic linear prediction architecture with shared weight encoders\relax }}{61}}
\newlabel{fig:arch1}{{6.2}{61}}
\newlabel{eqn:mag}{{6.2}{62}}
\newlabel{eqn:phase}{{6.3}{63}}
\newlabel{eqn:magpred}{{6.5}{63}}
\newlabel{eqn:phasepred}{{6.5}{63}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.2}Addressing Uncertainty}{64}}
\newlabel{subsec:uncertainty}{{6.3.2}{64}}
\newlabel{eqn:delta}{{6.6}{64}}
\newlabel{eqn:delta_loss}{{6.7}{65}}
\@writefile{loa}{\defcounter{refsection}{0}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Minibatch stochastic gradient descent training for prediction with uncertainty. The number of $\delta $-gradient descent steps ($k$) is treated as a hyper-parameter. \relax }}{65}}
\citation{slowAE}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {section}{\numberline {6.4}Experiments}{66}}
\newlabel{sec:experiments}{{6.4}{66}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.1}Shallow Architecture Trained on Natural Data}{66}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.2}Deep Architecture trained on NORB}{67}}
\newlabel{subsec:expr2}{{6.4.2}{67}}
\@writefile{lot}{\defcounter{refsection}{0}}
\@writefile{lot}{\contentsline {table}{\numberline {6.1}{\ignorespaces Summary of architectures\relax }}{68}}
\newlabel{tbl:arch}{{6.1}{68}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces Decoder filters learned by shallow phase-pooling architectures\relax }}{69}}
\newlabel{fig:filters}{{6.3}{69}}
\newlabel{fig:sample}{{6.4a}{69}}
\newlabel{sub@fig:sample}{{a}{69}}
\newlabel{fig:baseline}{{6.4b}{69}}
\newlabel{sub@fig:baseline}{{b}{69}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces (a) Test samples input to the network (b) Linear interpolation in code space learned by our Siamese-encoder network\relax }}{69}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.3}Uncertainty}{70}}
\newlabel{fig:noReg}{{6.5a}{71}}
\newlabel{sub@fig:noReg}{{a}{71}}
\newlabel{fig:withReg}{{6.5b}{71}}
\newlabel{sub@fig:withReg}{{b}{71}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.5}{\ignorespaces Linear interpolation in code space learned by our model. (a) no phase-pooling, no curvature regularization, (b) with phase pooling and curvature regularization\relax }}{71}}
\newlabel{fig:noDelta}{{6.6a}{71}}
\newlabel{sub@fig:noDelta}{{a}{71}}
\newlabel{fig:withDelta}{{\caption@xref {fig:withDelta}{ on input line 323}}{71}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.6}{\ignorespaces Interpolation results obtained by minimizing (a) Equation 6.1\hbox {} and (b) Equation 6.7\hbox {} trained with only partially predictable simulated video\relax }}{71}}
\newlabel{fig:delta}{{6.6}{71}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Conclusion}{73}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter{refsection}{0}}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:conclusion}{{7}{73}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\setcounter {tocdepth}{+1}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\setcounter {tocdepth}{+1}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{74}}
