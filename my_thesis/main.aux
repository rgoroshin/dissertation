\relax 
\bibstyle{biblatex}
\bibdata{main-blx,references}
\citation{biblatex-control} 
\@writefile{toc}{\boolfalse{citerequest}\boolfalse{citetracker}\boolfalse{pagetracker}}
\@writefile{lof}{\boolfalse{citerequest}\boolfalse{citetracker}\boolfalse{pagetracker}}
\@writefile{lot}{\boolfalse{citerequest}\boolfalse{citetracker}\boolfalse{pagetracker}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {section}{Dedication}{ii}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {section}{Acknowledgements}{iii}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {section}{Abstract}{iv}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {section}{List of Figures}{vii}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {section}{List of Tables}{xi}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter{refsection}{0}}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chaper:introduction}{{1}{1}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Related Work}{2}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter{refsection}{0}}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:related_work}{{2}{2}}
\citation{DHS}
\citation{SC}
\citation{LISTA}
\citation{SAE1}
\citation{SAE2}
\citation{CAE}
\citation{DAE}
\citation{DHS}
\citation{CAE}
\citation{SAE1}
\citation{SAE2}
\citation{bengio_new}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Saturating Auto-Encoders}{3}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter{refsection}{0}}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:SATAE}{{3}{3}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Introduction}{3}}
\citation{SAE1}
\citation{SAE2}
\citation{CAE}
\citation{CAE}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Latent State Regularization}{4}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Saturating Auto-Encoder through Complementary Nonlinearities}{5}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Three nonlinearities (top) with their associated complementary regularization functions(bottom).\relax }}{7}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:nonlin}{{3.1}{7}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Effect of the Saturation Regularizer}{8}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Visualizing the Energy Landscape}{8}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Energy surfaces for unregularized (left), and regularized (right) solutions obtained on SATAE-shrink and 10 basis vectors. Black corresponds to low reconstruction energy. Training points lie on a one-dimensional manifold shown in yellow.\relax }}{9}}
\newlabel{fig:toyshrink}{{3.2}{9}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}SATAE-shrink}{9}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces SATAE-SL toy example with two basis elements. Top Row: three randomly initialized solutions obtained with no regularization. Bottom Row: three randomly initialized solutions obtained with regularization.\relax }}{10}}
\newlabel{fig:toysatlinear}{{3.3}{10}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Geometric visualization of non-linearities\relax }}{10}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Evolution of two filters with increasing saturation regularization for a SATAE-SL trained on CIFAR-10. Filters corresponding to larger values of $\alpha $ were initialized using the filter corresponding to the previous $\alpha $. The regularization parameter was varied from 0.1 to 0.5 (left to right) in the top five images and 0.5 to 1 in the bottom five \relax }}{11}}
\newlabel{fig:horse}{{3.5}{11}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 5\p@ plus2.5\p@ minus\p@ \topsep 10\p@ plus4\p@ minus6\p@ \itemsep 5\p@ plus2.5\p@ minus\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Basis elements learned by the SATAE using different nonlinearities on: 28x28 binary MNIST digits, 12x12 gray scale natural image patches, and CIFAR-10. (a) SATAE-shrink trained on MNIST, (b) SATAE-saturated-linear trained on MNIST, (c) SATAE-shrink trained on natural image patches, (d) SATAE-saturated-linear trained on natural image patches, (e)-(f) SATAE-shrink trained on CIFAR-10 with $\alpha =0.1$ and $\alpha =0.5$, respectively, (g)-(h) SATAE-SL trained on CIFAR-10 with $\alpha =0.1$ and $\alpha =0.6$, respectively. \relax }}{12}}
\newlabel{fig:results}{{3.6}{12}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}SATAE-saturated-linear}{13}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.4}Experiments on CIFAR-10}{13}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Experimental Details}{14}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Discussion}{15}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.1}Extension to Differentiable Functions}{16}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces Illustration of the complimentary function ($f_c$) as defined by Equation 3 for a non-monotonic activation function ($f$). The absolute derivative of $f$ is shown for comparison.\relax }}{17}}
\newlabel{fig:diff_cc}{{3.7}{17}}
\citation{CAE}
\citation{LISTA}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.2}Relationship with the Contractive Auto-Encoder}{18}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.3}Relationship with the Sparse Auto-Encoder}{18}}
\citation{SC}
\citation{SAE}
\citation{SAE2}
\citation{ConvSC}
\citation{FISTA}
\citation{LISTA}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Convolutional Sparse Inference}{20}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter{refsection}{0}}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:LISTA}{{4}{20}}
\citation{ConvSC}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces LISTA network architecture\relax }}{21}}
\newlabel{fig:LISTA}{{4.1}{21}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Convolutional-LISTA}{21}}
\newlabel{eqn:lasso}{{4.1}{21}}
\citation{groupSparsity}
\citation{LISTA}
\citation{LISTA}
\citation{groupSparsity}
\citation{ConvSC}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Learning to Perform Sparse Inference}{22}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Pre-trained decoder used for fixed-decoder experiments\relax }}{23}}
\newlabel{fig:FISTA_decoder}{{4.2}{23}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Whitened CIFAR-10 samples (top) and their corresponding reconstructions.\relax }}{23}}
\newlabel{fig:CIFAR_rec}{{4.3}{23}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Sparse codes obtained with FISTA inference\relax }}{24}}
\newlabel{fig:activations}{{4.4}{24}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Sparse Inference in a Fixed Dictionary}{25}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces Sparse inference learning curves\relax }}{26}}
\newlabel{fig:learning_curves}{{4.5}{26}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces Lasso Loss with Fixed Dictionary\relax }}{27}}
\newlabel{fig:lasso_loss_fixed}{{4.6}{27}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces Reconstruction and Sparsity with Fixed Dictionary\relax }}{27}}
\newlabel{fig:scatter_fixed}{{4.7}{27}}
\citation{SAE}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces Lasso Loss with Learned Dictionary\relax }}{28}}
\newlabel{fig:lasso_loss_learned}{{4.8}{28}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Sparse Convolutional Auto-encoders}{28}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.9}{\ignorespaces Reconstruction and Sparsity with Learned Dictionary\relax }}{29}}
\newlabel{fig:scatter_learned}{{4.9}{29}}
\citation{SFA}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Learning Spatiotemporally Coherent Metrics}{30}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter{refsection}{0}}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:slow}{{5}{30}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Introduction}{30}}
\citation{DrLIM}
\citation{DrLIMVideo}
\citation{SFA}
\citation{Bengio2012}
\citation{CAE}
\citation{DAE}
\citation{SATAE}
\citation{LeCun1998}
\citation{JoanScat}
\citation{SFA}
\citation{SSA}
\citation{hyvarinen2003bubbles}
\citation{SFA}
\citation{complexCells}
\citation{complexCells}
\citation{DrLIM}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Contributions and Prior Work}{32}}
\citation{groupSparsity}
\citation{complexCells}
\citation{Cadieu}
\citation{zou2012deep}
\citation{Huyvarinen}
\citation{SFA}
\citation{SSA}
\citation{complexCells}
\citation{DrLIM}
\newlabel{fig:toyplane}{{5.1a}{34}}
\newlabel{sub@fig:toyplane}{{a}{34}}
\newlabel{fig:drlim}{{5.1b}{34}}
\newlabel{sub@fig:drlim}{{b}{34}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces (a) Three samples from our rotating plane toy dataset. (b) Scatter plot of the dataset plotted in the output space of $G_W$ at the start (top) and end (bottom) of training. The left side of the figure is colored by the yaw angle, and the right side by roll, $0^{\circ }$ blue, $90^{\circ }$ in pink.\relax }}{34}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Slowness as Metric Learning }{34}}
\newlabel{sec:slowmetric}{{5.3}{34}}
\citation{siamese}
\newlabel{eqn:drlimcrit}{{5.1}{35}}
\citation{JoanScat}
\citation{LeCun1998}
\citation{ImageNet}
\citation{JoanPooling}
\citation{groupSparsity}
\citation{LISTA}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Slow Feature Pooling Auto-Encoders}{36}}
\newlabel{sfautoencs}{{5.4}{36}}
\newlabel{eqn:loss}{{5.2}{37}}
\citation{LeCun1998}
\citation{MaxOut}
\newlabel{fig:pooldec}{{5.2a}{38}}
\newlabel{sub@fig:pooldec}{{a}{38}}
\newlabel{fig:pooll1dec}{{5.2b}{38}}
\newlabel{sub@fig:pooll1dec}{{b}{38}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Pooled decoder dictionaries learned without (a) and with (b) the $L_1$ penalty using (6.1\hbox {}).\relax }}{38}}
\newlabel{fig:sfpool}{{5.2}{38}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.1}Fully-Connected Architecture}{38}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.2}Convolutional Architecture}{39}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Block diagram of the Siamese convolutional model trained on pairs of frames. \relax }}{40}}
\newlabel{fig:diagram}{{5.3}{40}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Experimental Results}{40}}
\citation{alexthesis}
\citation{Bengio2012}
\citation{groupSparsity}
\newlabel{eqn:groupL1}{{5.3}{42}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces Six scenes from our YouTube dataset \relax }}{43}}
\newlabel{fig:youtube}{{5.4}{43}}
\newlabel{fig:drlimfilters}{{5.5a}{43}}
\newlabel{sub@fig:drlimfilters}{{a}{43}}
\newlabel{fig:L1filters}{{5.5b}{43}}
\newlabel{sub@fig:L1filters}{{b}{43}}
\newlabel{fig:groupL1filters}{{5.5c}{43}}
\newlabel{sub@fig:groupL1filters}{{c}{43}}
\newlabel{fig:slowfilters}{{5.5d}{43}}
\newlabel{sub@fig:slowfilters}{{d}{43}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces Pooled convolutional dictionaries (decoders) learned with: (a) DrLIM and (b) sparsity only, (c) group sparsity, and (d) sparsity and slowness. Groups of four features that were pooled together are depicted as horizontally adjacent filters.\relax }}{43}}
\newlabel{fig:filters}{{5.5}{43}}
\newlabel{fig:videoquery}{{5.6a}{44}}
\newlabel{sub@fig:videoquery}{{a}{44}}
\newlabel{fig:cifarquery}{{5.6b}{44}}
\newlabel{sub@fig:cifarquery}{{b}{44}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces Query results in the (a) video and (b) CIFAR-10 datasets. Each row corresponds to a different feature space in which the queries were performed; numbers (1 or 2) denote the number of convolution-pooling layers. \relax }}{44}}
\newlabel{fig:query}{{5.6}{44}}
\newlabel{fig:query}{{5.6}{44}}
\newlabel{fig:ROCtime}{{5.7a}{44}}
\newlabel{sub@fig:ROCtime}{{a}{44}}
\newlabel{fig:ROCCIFAR}{{5.7b}{44}}
\newlabel{sub@fig:ROCCIFAR}{{b}{44}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces Precision-Recall curves corresponding to the YouTube (a) and CIFAR-10 (b) dataset.\relax }}{44}}
\newlabel{fig:ROC}{{5.7}{44}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {section}{\numberline {5.6}Conclusion}{46}}
\citation{imageNetTransfer}
\citation{supFromTracker}
\citation{FBvideo}
\citation{predAlexNet}
\citation{Bengio2012}
\citation{sparseCoding}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Learning to Linearize under Uncertainty}{48}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter{refsection}{0}}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:error}{{6}{48}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Introduction}{48}}
\citation{FBvideo}
\citation{supFromTracker}
\citation{predAlexNet}
\citation{capsules}
\citation{taco}
\citation{FBvideo}
\citation{supFromTracker}
\citation{predAlexNet}
\citation{slowAE}
\citation{predAlexNet}
\citation{ImageNet}
\citation{supFromTracker}
\citation{slowAE}
\citation{SSA}
\citation{Cadieu}
\citation{SFA}
\citation{DrLIMVideo}
\citation{FBvideo}
\citation{ranzato2007}
\citation{zeiler2010}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Prior Work}{50}}
\newlabel{sec:prior work}{{6.2}{50}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Learning Linearized Representations}{51}}
\newlabel{sec:main}{{6.3}{51}}
\newlabel{fig:3pixels}{{6.1a}{52}}
\newlabel{sub@fig:3pixels}{{a}{52}}
\newlabel{fig:manifold}{{6.1b}{52}}
\newlabel{sub@fig:manifold}{{b}{52}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces (a) A video generated by translating a Gaussian intensity bump over a three pixel array ($x$,$y$,$z$), (b) the corresponding manifold parametrized by time in three dimensional space\relax }}{52}}
\newlabel{fig:threepixel}{{6.1}{52}}
\newlabel{eqn:loss}{{6.1}{52}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.1}Phase Pooling}{52}}
\newlabel{subsec:phase pooling}{{6.3.1}{52}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces The basic linear prediction architecture with shared weight encoders\relax }}{53}}
\newlabel{fig:arch1}{{6.2}{53}}
\newlabel{eqn:mag}{{6.2}{54}}
\newlabel{eqn:phase}{{6.3}{55}}
\newlabel{eqn:magpred}{{6.5}{55}}
\newlabel{eqn:phasepred}{{6.5}{55}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.2}Addressing Uncertainty}{56}}
\newlabel{subsec:uncertainty}{{6.3.2}{56}}
\newlabel{eqn:delta}{{6.6}{56}}
\newlabel{eqn:delta_loss}{{6.7}{57}}
\@writefile{loa}{\defcounter{refsection}{0}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Minibatch stochastic gradient descent training for prediction with uncertainty. The number of $\delta $-gradient descent steps ($k$) is treated as a hyper-parameter. \relax }}{57}}
\citation{slowAE}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {section}{\numberline {6.4}Experiments}{58}}
\newlabel{sec:experiments}{{6.4}{58}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.1}Shallow Architecture Trained on Natural Data}{58}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.2}Deep Architecture trained on NORB}{59}}
\newlabel{subsec:expr2}{{6.4.2}{59}}
\@writefile{lot}{\defcounter{refsection}{0}}
\@writefile{lot}{\contentsline {table}{\numberline {6.1}{\ignorespaces Summary of architectures\relax }}{60}}
\newlabel{tbl:arch}{{6.1}{60}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces Decoder filters learned by shallow phase-pooling architectures\relax }}{61}}
\newlabel{fig:filters}{{6.3}{61}}
\newlabel{fig:sample}{{6.4a}{61}}
\newlabel{sub@fig:sample}{{a}{61}}
\newlabel{fig:baseline}{{6.4b}{61}}
\newlabel{sub@fig:baseline}{{b}{61}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces (a) Test samples input to the network (b) Linear interpolation in code space learned by our Siamese-encoder network\relax }}{61}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.3}Uncertainty}{62}}
\newlabel{fig:noReg}{{6.5a}{63}}
\newlabel{sub@fig:noReg}{{a}{63}}
\newlabel{fig:withReg}{{6.5b}{63}}
\newlabel{sub@fig:withReg}{{b}{63}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.5}{\ignorespaces Linear interpolation in code space learned by our model. (a) no phase-pooling, no curvature regularization, (b) with phase pooling and curvature regularization\relax }}{63}}
\newlabel{fig:noDelta}{{6.6a}{63}}
\newlabel{sub@fig:noDelta}{{a}{63}}
\newlabel{fig:withDelta}{{\caption@xref {fig:withDelta}{ on input line 323}}{63}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.6}{\ignorespaces Interpolation results obtained by minimizing (a) Equation 6.1\hbox {} and (b) Equation 6.7\hbox {} trained with only partially predictable simulated video\relax }}{63}}
\newlabel{fig:delta}{{6.6}{63}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Adversarial Inpainting}{65}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter{refsection}{0}}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:uncertainty}{{7}{65}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {chapter}{\numberline {8}Conclusion}{66}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter{refsection}{0}}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:conclusion}{{8}{66}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\setcounter {tocdepth}{+1}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\setcounter {tocdepth}{+1}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{67}}
