\relax 
\bibstyle{biblatex}
\bibdata{main-blx,references}
\citation{biblatex-control} 
\@writefile{toc}{\boolfalse{citerequest}\boolfalse{citetracker}\boolfalse{pagetracker}}
\@writefile{lof}{\boolfalse{citerequest}\boolfalse{citetracker}\boolfalse{pagetracker}}
\@writefile{lot}{\boolfalse{citerequest}\boolfalse{citetracker}\boolfalse{pagetracker}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {section}{Dedication}{ii}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {section}{Acknowledgements}{iii}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {section}{Abstract}{iv}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {section}{List of Figures}{vii}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {section}{List of Tables}{xi}}
\citation{simoncelli2001}
\citation{bengio2013}
\citation{tenenbaum2000}
\citation{roweis2000}
\citation{goroshin2015}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter{refsection}{0}}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:introduction}{{1}{1}}
\citation{SIFT}
\citation{HoG}
\citation{gist}
\citation{fukushima1980}
\citation{LeCun1998}
\citation{ImageNet}
\citation{LeCun1998}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Top left: random image, Top right: natural image, Bottom left: visualization of unstructured data, Bottom right: visualization of data with intrinsic structure\relax }}{2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:structure}{{1.1}{2}}
\citation{hubel1968}
\citation{felleman1991}
\citation{yamins2014}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces The LeNet5 network was trained to recognize hand written characters in order to automatically process bank checks\relax }}{3}}
\newlabel{fig:LeNet5}{{1.2}{3}}
\citation{foldiak1991}
\citation{sinha2013}
\citation{sinha2013}
\citation{sharma2000}
\citation{yosinski2014}
\citation{SAE}
\citation{CAE}
\citation{LISTA}
\citation{SFA}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Thesis Outline and Summary of Contributions}{4}}
\citation{PCA}
\citation{nair2008}
\citation{capsules}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Related Work}{6}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter{refsection}{0}}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:related_work}{{2}{6}}
\@writefile{lot}{\defcounter{refsection}{0}}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces Summary of unsupervised feature learning algorithms and their properties\relax }}{7}}
\newlabel{tbl:models}{{2.1}{7}}
\citation{CBP}
\citation{ICA}
\citation{PCA}
\citation{SC}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Left: visualization of the optimization problem used to solve for independent representations. Right: visualization of the optimization problem used to solve for sparse representations.\relax }}{9}}
\newlabel{fig:ICA_lasso}{{2.1}{9}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Principal and Independent Component Analysis}{9}}
\citation{SC2}
\citation{SC2}
\citation{candes2006}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Sparse coding learns a local linear model of the data manifold (image from \cite {SC2}).\relax }}{10}}
\newlabel{fig:ICA_features}{{2.2}{10}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Sparse Representations}{10}}
\citation{BP}
\citation{FISTA}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Independent features learned on natural image patches\relax }}{11}}
\newlabel{fig:ICA_features}{{2.3}{11}}
\newlabel{eqn:lasso}{{2.2}{11}}
\citation{SAE1}
\citation{SAE2}
\citation{SC}
\citation{gabor}
\citation{groupSparsity}
\newlabel{eqn:sae_loss}{{2.3}{12}}
\citation{groupSparsity}
\citation{groupSparsity}
\citation{DHS}
\citation{SC}
\citation{LISTA}
\citation{SAE1}
\citation{SAE2}
\citation{CAE}
\citation{DAE}
\newlabel{eqn:gsc_loss}{{2.4}{13}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Features learned with group sparsity model in a two dimensional topology, with a local pooling neighborhood is overlaid in black. Figure from \cite {groupSparsity}.\relax }}{14}}
\newlabel{fig:GSC_features}{{2.4}{14}}
\citation{lecun2006}
\citation{CAE}
\citation{DAE}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Auto-Encoders and Energy Based Learning}{15}}
\citation{tenenbaum2000}
\citation{DrLIM}
\citation{tenenbaum2000}
\citation{coifman2006}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Metric Learning}{16}}
\citation{DHS}
\citation{CAE}
\citation{SAE1}
\citation{SAE2}
\citation{bengio_new}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Saturating Auto-Encoders}{18}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter{refsection}{0}}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:SATAE}{{3}{18}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Introduction}{18}}
\citation{SAE1}
\citation{SAE2}
\citation{CAE}
\citation{CAE}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Latent State Regularization}{19}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Saturating Auto-Encoder through Complementary Nonlinearities}{20}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Three nonlinearities (top) with their associated complementary regularization functions(bottom).\relax }}{22}}
\newlabel{fig:nonlin}{{3.1}{22}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Effect of the Saturation Regularizer}{22}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Visualizing the Energy Landscape}{22}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}SATAE-shrink}{23}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Energy surfaces for unregularized (left), and regularized (right) solutions obtained on SATAE-shrink and 10 basis vectors. Black corresponds to low reconstruction energy. Training points lie on a one-dimensional manifold shown in yellow.\relax }}{24}}
\newlabel{fig:toyshrink}{{3.2}{24}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces SATAE-SL toy example with two basis elements. Top Row: three randomly initialized solutions obtained with no regularization. Bottom Row: three randomly initialized solutions obtained with regularization.\relax }}{24}}
\newlabel{fig:toysatlinear}{{3.3}{24}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Geometric visualization of non-linearities\relax }}{25}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Evolution of two filters with increasing saturation regularization for a SATAE-SL trained on CIFAR-10. Filters corresponding to larger values of $\alpha $ were initialized using the filter corresponding to the previous $\alpha $. The regularization parameter was varied from 0.1 to 0.5 (left to right) in the top five images and 0.5 to 1 in the bottom five \relax }}{25}}
\newlabel{fig:horse}{{3.5}{25}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}SATAE-saturated-linear}{25}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 5\p@ plus2.5\p@ minus\p@ \topsep 10\p@ plus4\p@ minus6\p@ \itemsep 5\p@ plus2.5\p@ minus\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Basis elements learned by the SATAE using different nonlinearities on: 28x28 binary MNIST digits, 12x12 gray scale natural image patches, and CIFAR-10. (a) SATAE-shrink trained on MNIST, (b) SATAE-saturated-linear trained on MNIST, (c) SATAE-shrink trained on natural image patches, (d) SATAE-saturated-linear trained on natural image patches, (e)-(f) SATAE-shrink trained on CIFAR-10 with $\alpha =0.1$ and $\alpha =0.5$, respectively, (g)-(h) SATAE-SL trained on CIFAR-10 with $\alpha =0.1$ and $\alpha =0.6$, respectively. \relax }}{26}}
\newlabel{fig:results}{{3.6}{26}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.4}Experiments on CIFAR-10}{27}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Experimental Details}{28}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Discussion}{29}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.1}Extension to Differentiable Functions}{29}}
\citation{CAE}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces Illustration of the complimentary function ($f_c$) as defined by Equation 3 for a non-monotonic activation function ($f$). The absolute derivative of $f$ is shown for comparison.\relax }}{31}}
\newlabel{fig:diff_cc}{{3.7}{31}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.2}Relationship with the Contractive Auto-Encoder}{31}}
\citation{LISTA}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.3}Relationship with the Sparse Auto-Encoder}{32}}
\citation{SC}
\citation{SAE}
\citation{SAE2}
\citation{ConvSC}
\citation{FISTA}
\citation{LISTA}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Convolutional Sparse Inference}{33}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter{refsection}{0}}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:LISTA}{{4}{33}}
\citation{ConvSC}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces LISTA network architecture\relax }}{34}}
\newlabel{fig:LISTA}{{4.1}{34}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Convolutional-LISTA}{34}}
\newlabel{eqn:lasso}{{4.1}{34}}
\citation{groupSparsity}
\citation{LISTA}
\citation{LISTA}
\citation{groupSparsity}
\citation{ConvSC}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Learning to Perform Sparse Inference}{35}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Pre-trained decoder used for fixed-decoder experiments\relax }}{36}}
\newlabel{fig:FISTA_decoder}{{4.2}{36}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Whitened CIFAR-10 samples (top) and their corresponding reconstructions.\relax }}{36}}
\newlabel{fig:CIFAR_rec}{{4.3}{36}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Sparse codes obtained with FISTA inference\relax }}{37}}
\newlabel{fig:activations}{{4.4}{37}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Sparse Inference in a Fixed Dictionary}{38}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces Sparse inference learning curves\relax }}{39}}
\newlabel{fig:learning_curves}{{4.5}{39}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces Lasso Loss with Fixed Dictionary\relax }}{40}}
\newlabel{fig:lasso_loss_fixed}{{4.6}{40}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces Reconstruction and Sparsity with Fixed Dictionary\relax }}{40}}
\newlabel{fig:scatter_fixed}{{4.7}{40}}
\citation{SAE}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces Lasso Loss with Learned Dictionary\relax }}{41}}
\newlabel{fig:lasso_loss_learned}{{4.8}{41}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Sparse Convolutional Auto-encoders}{41}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.9}{\ignorespaces Reconstruction and Sparsity with Learned Dictionary\relax }}{42}}
\newlabel{fig:scatter_learned}{{4.9}{42}}
\citation{SFA}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Learning Spatiotemporally Coherent Metrics}{43}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter{refsection}{0}}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:slow}{{5}{43}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Introduction}{43}}
\citation{DrLIM}
\citation{DrLIMVideo}
\citation{SFA}
\citation{Bengio2012}
\citation{CAE}
\citation{DAE}
\citation{SATAE}
\citation{LeCun1998}
\citation{JoanScat}
\citation{SFA}
\citation{SSA}
\citation{hyvarinen2003bubbles}
\citation{SFA}
\citation{complexCells}
\citation{complexCells}
\citation{DrLIM}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Contributions and Prior Work}{45}}
\citation{groupSparsity}
\citation{complexCells}
\citation{Cadieu}
\citation{zou2012deep}
\citation{Huyvarinen}
\citation{SFA}
\citation{SSA}
\citation{complexCells}
\citation{DrLIM}
\newlabel{fig:toyplane}{{5.1a}{47}}
\newlabel{sub@fig:toyplane}{{a}{47}}
\newlabel{fig:drlim}{{5.1b}{47}}
\newlabel{sub@fig:drlim}{{b}{47}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces (a) Three samples from our rotating plane toy dataset. (b) Scatter plot of the dataset plotted in the output space of $G_W$ at the start (top) and end (bottom) of training. The left side of the figure is colored by the yaw angle, and the right side by roll, $0^{\circ }$ blue, $90^{\circ }$ in pink.\relax }}{47}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Slowness as Metric Learning }{47}}
\newlabel{sec:slowmetric}{{5.3}{47}}
\citation{siamese}
\newlabel{eqn:drlimcrit}{{5.1}{48}}
\citation{JoanScat}
\citation{LeCun1998}
\citation{ImageNet}
\citation{JoanPooling}
\citation{groupSparsity}
\citation{LISTA}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Slow Feature Pooling Auto-Encoders}{49}}
\newlabel{sfautoencs}{{5.4}{49}}
\newlabel{eqn:loss}{{5.2}{50}}
\citation{LeCun1998}
\citation{MaxOut}
\newlabel{fig:pooldec}{{5.2a}{51}}
\newlabel{sub@fig:pooldec}{{a}{51}}
\newlabel{fig:pooll1dec}{{5.2b}{51}}
\newlabel{sub@fig:pooll1dec}{{b}{51}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Pooled decoder dictionaries learned without (a) and with (b) the $L_1$ penalty using (6.1\hbox {}).\relax }}{51}}
\newlabel{fig:sfpool}{{5.2}{51}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.1}Fully-Connected Architecture}{51}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.2}Convolutional Architecture}{52}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Block diagram of the Siamese convolutional model trained on pairs of frames. \relax }}{53}}
\newlabel{fig:diagram}{{5.3}{53}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Experimental Results}{53}}
\citation{alexthesis}
\citation{Bengio2012}
\citation{groupSparsity}
\newlabel{eqn:groupL1}{{5.3}{55}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces Six scenes from our YouTube dataset \relax }}{56}}
\newlabel{fig:youtube}{{5.4}{56}}
\newlabel{fig:drlimfilters}{{5.5a}{56}}
\newlabel{sub@fig:drlimfilters}{{a}{56}}
\newlabel{fig:L1filters}{{5.5b}{56}}
\newlabel{sub@fig:L1filters}{{b}{56}}
\newlabel{fig:groupL1filters}{{5.5c}{56}}
\newlabel{sub@fig:groupL1filters}{{c}{56}}
\newlabel{fig:slowfilters}{{5.5d}{56}}
\newlabel{sub@fig:slowfilters}{{d}{56}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces Pooled convolutional dictionaries (decoders) learned with: (a) DrLIM and (b) sparsity only, (c) group sparsity, and (d) sparsity and slowness. Groups of four features that were pooled together are depicted as horizontally adjacent filters.\relax }}{56}}
\newlabel{fig:filters}{{5.5}{56}}
\@writefile{lot}{\defcounter{refsection}{0}}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces Comparison of Temporal and Class AUCs\relax }}{56}}
\newlabel{fig:videoquery}{{5.6a}{57}}
\newlabel{sub@fig:videoquery}{{a}{57}}
\newlabel{fig:cifarquery}{{5.6b}{57}}
\newlabel{sub@fig:cifarquery}{{b}{57}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces Query results in the (a) video and (b) CIFAR-10 datasets. Each row corresponds to a different feature space in which the queries were performed; numbers (1 or 2) denote the number of convolution-pooling layers. \relax }}{57}}
\newlabel{fig:query}{{5.6}{57}}
\newlabel{fig:query}{{5.6}{57}}
\newlabel{fig:ROCtime}{{5.7a}{57}}
\newlabel{sub@fig:ROCtime}{{a}{57}}
\newlabel{fig:ROCCIFAR}{{5.7b}{57}}
\newlabel{sub@fig:ROCCIFAR}{{b}{57}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces Precision-Recall curves corresponding to the YouTube (a) and CIFAR-10 (b) dataset.\relax }}{57}}
\newlabel{fig:ROC}{{5.7}{57}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {section}{\numberline {5.6}Conclusion}{59}}
\citation{imageNetTransfer}
\citation{supFromTracker}
\citation{FBvideo}
\citation{predAlexNet}
\citation{Bengio2012}
\citation{sparseCoding}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Learning to Linearize under Uncertainty}{61}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter{refsection}{0}}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:linear}{{6}{61}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Introduction}{61}}
\citation{FBvideo}
\citation{supFromTracker}
\citation{predAlexNet}
\citation{capsules}
\citation{taco}
\citation{FBvideo}
\citation{supFromTracker}
\citation{predAlexNet}
\citation{slowAE}
\citation{predAlexNet}
\citation{ImageNet}
\citation{supFromTracker}
\citation{slowAE}
\citation{SSA}
\citation{Cadieu}
\citation{SFA}
\citation{DrLIMVideo}
\citation{FBvideo}
\citation{ranzato2007}
\citation{zeiler2010}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Prior Work}{63}}
\newlabel{sec:prior work}{{6.2}{63}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Learning Linearized Representations}{64}}
\newlabel{sec:main}{{6.3}{64}}
\newlabel{fig:3pixels}{{6.1a}{65}}
\newlabel{sub@fig:3pixels}{{a}{65}}
\newlabel{fig:manifold}{{6.1b}{65}}
\newlabel{sub@fig:manifold}{{b}{65}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces (a) A video generated by translating a Gaussian intensity bump over a three pixel array ($x$,$y$,$z$), (b) the corresponding manifold parametrized by time in three dimensional space\relax }}{65}}
\newlabel{fig:threepixel}{{6.1}{65}}
\newlabel{eqn:loss}{{6.1}{65}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.1}Phase Pooling}{65}}
\newlabel{subsec:phase pooling}{{6.3.1}{65}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces The basic linear prediction architecture with shared weight encoders\relax }}{66}}
\newlabel{fig:arch1}{{6.2}{66}}
\newlabel{eqn:mag}{{6.2}{67}}
\newlabel{eqn:phase}{{6.3}{68}}
\newlabel{eqn:magpred}{{6.5}{68}}
\newlabel{eqn:phasepred}{{6.5}{68}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.2}Addressing Uncertainty}{69}}
\newlabel{subsec:uncertainty}{{6.3.2}{69}}
\newlabel{eqn:delta}{{6.6}{69}}
\newlabel{eqn:delta_loss}{{6.7}{70}}
\@writefile{loa}{\defcounter{refsection}{0}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Minibatch stochastic gradient descent training for prediction with uncertainty. The number of $\delta $-gradient descent steps ($k$) is treated as a hyper-parameter. \relax }}{70}}
\citation{slowAE}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {section}{\numberline {6.4}Experiments}{71}}
\newlabel{sec:experiments}{{6.4}{71}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.1}Shallow Architecture Trained on Natural Data}{71}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.2}Deep Architecture trained on NORB}{72}}
\newlabel{subsec:expr2}{{6.4.2}{72}}
\@writefile{lot}{\defcounter{refsection}{0}}
\@writefile{lot}{\contentsline {table}{\numberline {6.1}{\ignorespaces Summary of architectures\relax }}{73}}
\newlabel{tbl:arch}{{6.1}{73}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces Decoder filters learned by shallow phase-pooling architectures\relax }}{74}}
\newlabel{fig:filters}{{6.3}{74}}
\newlabel{fig:sample}{{6.4a}{74}}
\newlabel{sub@fig:sample}{{a}{74}}
\newlabel{fig:baseline}{{6.4b}{74}}
\newlabel{sub@fig:baseline}{{b}{74}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces (a) Test samples input to the network (b) Linear interpolation in code space learned by our Siamese-encoder network\relax }}{74}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.3}Uncertainty}{75}}
\newlabel{fig:noReg}{{6.5a}{76}}
\newlabel{sub@fig:noReg}{{a}{76}}
\newlabel{fig:withReg}{{6.5b}{76}}
\newlabel{sub@fig:withReg}{{b}{76}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.5}{\ignorespaces Linear interpolation in code space learned by our model. (a) no phase-pooling, no curvature regularization, (b) with phase pooling and curvature regularization\relax }}{76}}
\newlabel{fig:noDelta}{{6.6a}{76}}
\newlabel{sub@fig:noDelta}{{a}{76}}
\newlabel{fig:withDelta}{{\caption@xref {fig:withDelta}{ on input line 323}}{76}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.6}{\ignorespaces Interpolation results obtained by minimizing (a) Equation 6.1\hbox {} and (b) Equation 6.7\hbox {} trained with only partially predictable simulated video\relax }}{76}}
\newlabel{fig:delta}{{6.6}{76}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Conclusion}{78}}
\@writefile{lof}{\defcounter{refsection}{0}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter{refsection}{0}}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:conclusion}{{7}{78}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\setcounter {tocdepth}{+1}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\setcounter {tocdepth}{+1}}
\@writefile{toc}{\defcounter{refsection}{0}}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{79}}
