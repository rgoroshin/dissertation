% This material is a compilation of all the previous work sections from each of the four papers...  For that reason it's a little bit disjointed.

A large body of literature is devoted to real-time recovery of pose for marker-less articulable objects, such as human bodies, clothes, and man-made objects. A full review of all literature is outside the scope of this thesis. Instead we will discuss relevant prior work that relates directly to: offline construction of ground-truth pose datasets, hand pose recognition and localization of 2D human body joints from RGB and Depth image sources.

\subsection*{Geometric Model-Based Hand Tracking and Hand-Pose Datasets}

Many groups have created their own dataset of ground-truth labels and images to enable real-time pose recovery of the human hand. For example, Wang et al.~\cite{wang_pop_6d_hands} use the CyberGlove II Motion Capture system to construct a dataset of labeled hand poses from users, which are re-rendered as a colored glove with known-texture. A similar colored glove is worn by the user at run-time, and the pose is inferred in real-time by matching the imaged glove in RGB to their database of templates ~\cite{wang_pop_color_glove}. In later work, the CyberGlove data was repurposed for pose inference using template matching on depth images, without a colored glove. Wang et al. have recently commercialized their hand-tracking system (which was developed by 3Gear Systems~\cite{3Gear} as a proprietary framework, which is now owned and managed by Facebook Inc.) and now uses a PrimeSense depth camera oriented above the table to recognize a large range of possible poses. Our work differs from 3Gear's in a number of ways: 1) we attempt to perform continuous pose estimation rather than recognition by matching into a static and discrete database and 2) we orient the camera facing the user and so our system is optimized for a different set of hand gestures.

Also relevant to our work is that of Shotton et al.~\cite{shotton2013real}, who used randomized decision forests to recover the pose of multiple bodies from a single frame by learning a per-pixel classification of the depth image into 38 different body parts. Their training examples were synthesized from combinations of known poses and body shapes. Our hand-detection stage described in Part~\ref{part:one} can be seen as an implementation of the work by Shotton et al. on a restricted label set.

In similar work, Keskin et al.~\cite{keskin:2011} created a randomized decision forest classifier specialized for human hands. Lacking a dataset based on human motion capture, they synthesized a dataset from known poses in American Sign Language, and expanded the dataset by interpolating between poses. Owing to their prescribed goal of recognizing sign language signs themselves, this approach proved useful, but would not be feasible in our case as we require unrestricted hand poses to be recovered. In a follow on work~\cite{keskin:2012}, Keskin et al. presented a novel shape classification forest architecture to perform per-pixel part classification.

Several other groups have used domain-knowledge and temporal coherence to construct methods that do not require any dataset for tracking the pose of complicated objects. For example, Wiese et al.~\cite{weise09face} devise a real-time facial animation system for range sensors using salient points to deduce transformations on an underlying face model by framing it as energy minimization. In related work, Li et al.~\cite{hao_li_1} showed how to extend this technique to enable adaptation to the user's own facial expressions in an online fashion. Melax et al.~\cite{Melax:2013} demonstrate a real-time system for tracking the full pose of a human hand by fitting convex polyhedra directly to range data using an approach inspired by constraint-based physics systems. Ballan et al.~\cite{Ballan2012} show how to fit high polygon hand models to multiple camera views of a pair of hands interacting with a small sphere, using a combination of feature-based tracking and energy minimization. In contrast to our method, their approach relies upon inter-frame correspondences to provide optical-flow and good starting poses for energy minimization.

Early work by Rehg and Kanade~\cite{Rehg} demonstrated a model-based tracking system that fits a high-degree of freedom articulated hand model to greyscale image data using hand-designed 2D features. Zhao et al.~\cite{Zhao:2012} use a combination of IR markers and RGBD capture to infer offline (at one frame per second) the pose of an articulated hand model. Similar to this work, Oikonomidis et al.~\cite{bmvc2011oikonom} demonstrate the utility of Particle Swarm Optimization (PSO) for tracking single and interacting hands by searching for parameters of an explicit 3D model that reduce the reconstruction error of a z-buffer rendered model compared to an incoming depth image. Their work relies heavily on temporal coherence assumptions for efficient inference of the PSO optimizer, since the radius of convergence of their optimizer is finite. Unfortunately, temporal coherence cannot be relied on for robust real-time tracking since dropped frames and fast moving objects typically break this temporal coherency assumption. In contrast to their work, which used PSO directly for interactive tracking on the GPU at 4-15fps, our hand-tracking work shows that with relaxed temporal coherence assumptions in an offline setting, PSO is an invaluable \emph{offline} tool for generating labeled data.

\subsection*{ConvNet-Based Hand Tracking}

To our knowledge, our work of \cite{tompsonTOG14} - presented in Part~\ref{part:one} - was the first to use ConvNets to recover continuous 3D pose of human hands from depth data. However, several groups had shown ConvNets can recover the pose of rigid and non-rigid 3D objects such as plastic toys, faces and even human bodies. One of the earliest applications of ConvNets to hand-tracking was by Nowlan and Platt~\cite{nowlan}, which used them to recover the 2D center of mass of the human hand as well as recognize discrete hand gestures. This system was limited to a constrained laboratory environment. By contrast, our system can recover the full articulated pose of the hand in a broader range of environments.

LeCun et al.~\cite{lecun_toys} used ConvNets to deduce the 6 Degree Of Freedom (DOF) pose of 3D plastic toys by finding a low-dimensional embedding which maps RGB images to a six dimensional space. Osadchy et al.~\cite{lecun_face} use a similar formulation to perform pose detection of faces via a non-linear mapping to a low-dimensional manifold. Taylor et al.~\cite{TaylorSBF11} use crowd-sourcing to build a database of similar human poses from different subjects and then use ConvNets to perform dimensionality reduction to a low-dimensional manifold, where similarity between training examples is preserved. Lastly, Jiu et al.~\cite{Liris6327} use ConvNets to perform per-pixel classifications of depth images (whose output is similar to \cite{shotton2013real}) in order to infer human body pose, but they do not evaluate the performance of their approach on hand pose recognition.

Couprie et al.~\cite{couprie2013indoor} use ConvNets to perform image segmentation of indoor scenes using RGB-D data. The significance of their work is that it shows that ConvNets can perform high level reasoning from depth image features.

\subsection*{Geometric Model Based Body-Tracking}

One of the earliest works on articulated tracking in video was Hogg~\cite{hogg1983model} in 1983 using edge features and a simple cylinder based body model.  Several other model based articulated tracking systems have been reported over the past two decades, most notably~\cite{rehg1995model, kakadiaris1996model, wren1997pfinder, bregler1998tracking, deutscher2000articulated, sidenbladh2000stochastic, sminchisescu2001covariance}. The models used in these systems were explicit 2D or 3D jointed geometric models.  Most systems had to be hand-initialized (except \cite{wren1997pfinder}), and focused on incrementally updating pose parameters from one frame to the next. More complex examples come from the HumanEva dataset competitions~\cite{Sigal2010} that use video or higher-resolution shape models such as SCAPE~\cite{anguelov2005scape} and extensions. We refer the reader to~\cite{poppe2007vision} for a complete survey of this era.   

Most recently, such techniques have been shown to create very high-resolution animations of detailed body and cloth deformations \cite{de2008performance,jain2010moviereshape,stoll2011fast}.  Our approaches differ, since we are dealing with single view videos in unconstrained environments.

\subsection*{Statistical Based Body-Tracking}

One of the earliest systems that used no explicit geometric model was reported by Freeman et al. in 1995 \cite{freeman1995orientation} using oriented angle histograms to recognize hand configurations.  This was the precursor for the bag-of-features, SIFT~\cite{SIFT}, STIP~\cite{STIP}, HoG, and Histogram of Flow (HoF)~\cite{dalal2006human} approaches that boomed a decade later, most notably including the work by Dalal and Triggs in 2005 \cite{Dalal2005}.

For unconstrained image domains, many human body pose architectures have been proposed, including ``shape-context'' edge-based histograms from the human body \cite{mori2002estimating, agarwal2006recovering} or just silhouette features \cite{Grauman2003}.  Shakhnarovich et al. \cite{Shakhnarovich03fastpose} learn a parameter sensitive hash function to perform example-based pose estimation.  Many techniques have been proposed that extract, learn, or reason over entire body features.  Some use a combination of local detectors and structural reasoning~\cite{ramanan2005strike} for coarse tracking and \cite{buehler2009learning} for person-dependent tracking. 

Though the idea of using ``Pictorial Structures"  by Fischler and Elschlager~\cite{Fischler73} has been around since the 1970s, matching them efficiently to images has only been possible since the famous work on `Deformable Part Models' (DPM) by Felzenszwalb et al.~\cite{felzenszwalb2008discriminatively} in 2008. Subsequently many algorithms have been developed that use DPM for creating the body part unary distribution~\cite{andriluka2009pictorial, Eichner:2009:BAM, yang11cvpr, dantone13cvpr} with spatial-models incorporating body-part relationship priors. Algorithms which model more complex joint relationships, such as Yang and Ramanan~\cite{yang11cvpr}, use a flexible mixture of templates modeled by linear SVMs. Johnson and Everingham~\cite{johnson11cvpr}, who also proposed the `Leeds Sports Database' used for evaluation in this work, employ a cascade of body part detectors to obtain more discriminative templates. Most recent approaches aim to model higher-order part relationships. 

Almost all best performing algorithms since have solely built on HoG and DPM for local evidence, and yet more sophisticated spatial models. Pishchulin~\cite{pishchulin13cvpr, pishchulin13iccv} proposes a model that augments the DPM model with \emph{Poselet} conditioned priors of \cite{PoseletsICCV09} to capture spatial relationships of body-parts. Sapp and Taskar~\cite{modec} propose a multi-modal model which includes both holistic and local cues for mode selection and pose estimation, where they cluster images in the pose-space and then find the mode which best describes the input image. The pose of this mode then acts as a strong spatial prior, whereas the local evidence is again based on HoG and gradient features.

Following the \emph{Poselets} approach, the \emph{Armlets} approach by Gkioxari et al.~\cite{Gkioxari:2013:APE} employs a semi-global classifier for part configuration that incorporates edges, contours, and color histograms in addition to the HoG features. Their technique shows good performance on real-world data, however it is tested only on arms. 

A common drawback to all of these earlier models is that they suffer from the fact that they use hand crafted features such as HoG features, edges, contours, and color histograms. Our work, and the subsequent success of deep ConvNet architectures on the task of human body pose recognition, suggests that these hand-crafted features lack the discriminative power to infer the location of human joints in RGB and they exhibit poor generalization performance in unconstrained conditions, likely related to their inability to learn adequately invariant features.  Furthermore, our ConvNet formulation is able to jointly learn both the local features and the global structure.

Lastly, the previously mentioned work of Shotton et al.~\cite{shotton2013real} (discussed in the hand tracking section above) is also a relevant reference for the human body pose detection models described through Parts \ref{part:two}, \ref{part:three} and \ref{part:four}. Unlike the work of Shotton et al., our system estimates 2D body pose in RGB images without the use of depth data and in a wider variety of scenes and poses. As a means of reducing overall system latency and avoiding repeated false detections, the work of Shotton et al. focuses on pose inference using only a single depth image, whereas in our work (described in Part~\ref{part:three}) we use up to 2 frames of RGB data to significantly improve detection accuracy for ambiguous poses.

\subsection*{ConvNet Based Body-Tracking}

The current state-of-the-art methods for the task of human-pose estimation \emph{in-the-wild} are built using ConvNets~\cite{deeppose, jainiclr2014, tompsonnips2014, arjunaccv2014, chennips2014}.  Toshev et al.~\cite{deeppose} were the first to show that a variant of deep-learning was able to outperform state-of-art on the `FLIC' \cite{modec} dataset and was competitive with previous techniques on the `LSP' \cite{Johnson10} dataset. In contrast to our work, they formulate the problem as a direct (continuous) regression to joint location rather than a discrete heat-map output. However, their method performs poorly in the high-precision region and we believe that this is because the mapping from input RGB image to XY location adds unnecessary learning complexity which weakens generalization.

For example, direct regression does not deal gracefully with multi-modal outputs (where a valid joint is present in two spatial locations). Since the network is forced to produce a single output for a given regression input, the network does not have enough degrees of freedom in the output representation to afford small errors which we believe leads to over-training (since small outliers - due to for instance the presence of a valid body part - will contribute to a large error in XY).

Chen et al.~\cite{chennips2014} use a ConvNet to learn a low-dimensional representation of the input image and use an image dependent spatial model and show improvement over~\cite{deeppose}. In contrast to our work, the model of Part~\ref{part:two} (from \cite{tompsonnips2014, arjunaccv2014}) uses a multi-resolution ConvNet architecture to perform heat-map likelihood regression which we train jointly with a graphical model network to further promote joint consistency. This joint training enables our work to outperform \cite{chennips2014}.

In an unrelated application, Eigen et al.~\cite{eigen2014} predict depth by using a cascade of coarse to fine ConvNet models. In their work the coarse model is pre-trained and the model parameters are fixed when training the fine model. By contrast, our model presented in Part~\ref{part:four} proposes a novel shared-feature architecture which enables joint training of both models to improve generalization performance and which samples a subset of the feature inputs to improve runtime performance.

\subsection*{Joint Training of a ConvNet and Graphical Model}

Joint training of neural-networks and graphical models has been previously reported by Ning et al.~\cite{ning05} for image segmentation, and by various groups in speech and language modeling \cite{bourlard1995remap, Morin05}. To our knowledge no such model has been successfully used for the problem of detecting and localizing body part positions of humans in images.  Recently, Ross et al.~\cite{ross_learning_message_passing} use a message-passing inspired procedure for structured prediction on computer vision tasks, such as 3D point cloud classification and 3D surface estimation from single images. In contrast to this work, we formulate our message-parsing inspired network in a way that is more amenable to back-propagation and so can be implemented in existing neural networks. 

Heitz et al.~\cite{Heitz_cascadedclassification} train a cascade of off-the-shelf classifiers for simultaneously performing object detection, region labeling, and geometric reasoning. However, because of the forward nature of the cascade, a later classifier is unable to encourage earlier ones to focus its effort on fixing certain error modes, or allow the earlier classifiers to ignore mistakes that can be undone by classifiers further in the cascade. Bergtholdt et al.~\cite{bergtholdt2010study} propose an approach for object class detection using a parts-based model where they are able to create a fully connected graph on parts and perform MAP-inference using $A^{*}$ search, but rely on SIFT and color features to create the unary and pairwise potentials.

